{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c76de6a",
   "metadata": {},
   "source": [
    "# Transformer Block\n",
    "\n",
    "The Transformer block is a key component of the Transformer model, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al.\n",
    "\n",
    "The Transformer block consists of two main parts:\n",
    "\n",
    "1. **Multi-Head Self-Attention Mechanism**: This mechanism allows the model to focus on different positions of the input sequence when producing the output. It does this by applying the attention mechanism multiple times in parallel (hence \"multi-head\") to the input. The attention scores are calculated using the formula:\n",
    "\n",
    "    $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "    where $Q$, $K$, and $V$ are the query, key, and value vectors, and $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "2. **Position-wise Feed-Forward Networks**: These are fully connected feed-forward networks applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "The output of each sub-layer (Multi-Head Attention and Feed-Forward) is then passed through a residual connection followed by layer normalization.\n",
    "\n",
    "The Transformer block can be visualized as follows:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/transformer.png\" alt=\"Transformer Block Diagram\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "This architecture allows the Transformer to handle dependencies regardless of their distance in the input or output sequences, making it effective for a wide range of tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.text import BLEUScore  # Use torchmetrics for BLEU score\n",
    "import spacy\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# import glob  # No longer needed after removing old cleanup\n",
    "import tarfile  # Add tarfile for manual extraction\n",
    "\n",
    "# Import specific callbacks from lightning\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Import the Transformer model components from our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0431c6c",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Configuration\n",
    "\n",
    "### Seed for Reproducibility\n",
    "We set random seeds for Python's `random`, `numpy`, and `torch` to ensure that our results are reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d6feb",
   "metadata": {},
   "source": [
    "\n",
    "### Special tokens and Indices\n",
    "\n",
    "Define special tokens used in sequence processing:\n",
    "- `<pad>`: Padding token.\n",
    "- `<sos>`: Start of Sentence token.\n",
    "- `<eos>`: End of Sentence token.\n",
    "- `<unk>`: Unknown token (for words not in the vocabulary).\n",
    "\n",
    "We also define their corresponding fixed indices, ensuring `<pad>` is at index 0, which is often required by loss functions like `CrossEntropyLoss(ignore_index=...)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define special tokens and indices\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60136c1e",
   "metadata": {},
   "source": [
    "\n",
    "## Data Handling with PyTorch Lightning DataModule\n",
    "\n",
    "We define a `LightningDataModule` to encapsulate all data-related steps: downloading, tokenizing, building vocabulary, processing, and creating DataLoaders. This keeps the main training script clean and organized.\n",
    "\n",
    "### `TranslationDataset`\n",
    "A simple custom `Dataset` class to hold source and target tensor pairs.\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_item = self.src_data[idx]\n",
    "        tgt_item = self.tgt_data[idx]\n",
    "        return src_item, tgt_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c835d5",
   "metadata": {},
   "source": [
    "\n",
    "### `TransformerDataModule`\n",
    "This class handles:\n",
    "- **Initialization**: Sets batch size and data directory.\n",
    "- **Spacy Models**: Loads English and German `spaCy` models for tokenization, downloading them if necessary.\n",
    "- **Tokenization**: Defines `tokenize_de` and `tokenize_en` methods.\n",
    "- **`prepare_data`**: Downloads the Multi30k dataset and ensures spaCy models are ready. This runs only once.\n",
    "- **`setup`**: Builds vocabularies using `torchtext.vocab.build_vocab_from_iterator` and processes the raw data into tensors. This runs on every GPU/process in distributed settings.\n",
    "- **`_process_data`**: Converts text pairs into sequences of indices, adding `<sos>` and `<eos>` tokens to the target sequence.\n",
    "- **`_collate_fn`**: Pads sequences within each batch to the maximum length in that batch using `pad_sequence`. Ensures `batch_first=True`.\n",
    "- **Dataloaders**: Defines `train_dataloader`, `val_dataloader`, and `test_dataloader` methods to provide data batches to the trainer. Uses multiple workers for potentially faster loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d8e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "class TransformerDataModule(pl.LightningDataModule):\n",
    "    \"\"\"LightningDataModule for Multi30k dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int = 32, data_dir: str = \".data\"):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.spacy_en = None\n",
    "        self.spacy_de = None\n",
    "        self.de_vocab = None\n",
    "        self.en_vocab = None\n",
    "        self.train_data = None\n",
    "        self.valid_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "    def _load_spacy_models(self):\n",
    "        \"\"\"Loads spaCy models, downloading if necessary.\"\"\"\n",
    "        try:\n",
    "            self.spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "            self.spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy models...\")\n",
    "            os.system(\"python -m spacy download de_core_news_sm\")\n",
    "            os.system(\"python -m spacy download en_core_web_sm\")\n",
    "            self.spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "            self.spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_de(self, text):\n",
    "        \"\"\"Tokenize German text.\"\"\"\n",
    "        if self.spacy_de is None:\n",
    "            self._load_spacy_models()  # Ensure loaded\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text.lower())]\n",
    "\n",
    "    def tokenize_en(self, text):\n",
    "        \"\"\"Tokenize English text.\"\"\"\n",
    "        if self.spacy_en is None:\n",
    "            self._load_spacy_models()  # Ensure loaded\n",
    "        return [tok.text for tok in self.spacy_en.tokenizer(text.lower())]\n",
    "\n",
    "    def _yield_tokens(self, data_iter, tokenizer):\n",
    "        \"\"\"Helper function for build_vocab_from_iterator.\"\"\"\n",
    "        for text in data_iter:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download dataset and spaCy models, manually extracting test set to avoid corruption. Runs once.\"\"\"\n",
    "        # Remove the previous cleanup logic as it was ineffective before download\n",
    "\n",
    "        # Original prepare_data logic:\n",
    "        print(\"Proceeding with Multi30k download...\")\n",
    "        # This will download train, valid, test archives if not present\n",
    "        Multi30k(\n",
    "            root=self.data_dir,\n",
    "            split=(\n",
    "                \"train\",\n",
    "                \"valid\",\n",
    "                \"test\",\n",
    "            ),  # Need to request test split to ensure archive is downloaded\n",
    "            language_pair=(\"de\", \"en\"),\n",
    "        )\n",
    "\n",
    "        # --- Manually extract test.de and test.en to ensure correctness ---\n",
    "        # Corrected path based on list_dir results\n",
    "        data_root_dir = os.path.join(self.data_dir, \"datasets\", \"Multi30k\")\n",
    "        test_archive_name = \"mmt16_task1_test.tar.gz\"\n",
    "        test_archive_path = os.path.join(data_root_dir, test_archive_name)\n",
    "        correct_test_files = [\"test.de\", \"test.en\"]\n",
    "\n",
    "        if os.path.exists(test_archive_path):\n",
    "            print(\n",
    "                f\"Manually extracting required test files from {test_archive_path}...\"\n",
    "            )\n",
    "            try:\n",
    "                with tarfile.open(test_archive_path, \"r:gz\") as tar:\n",
    "                    extracted_count = 0\n",
    "                    for member in tar.getmembers():\n",
    "                        print(\n",
    "                            f\"  Found member in archive: {member.name} (Is file: {member.isfile()})\"\n",
    "                        )  # Log every member found\n",
    "                        # Check if the member is one of the files we want\n",
    "                        # Compare basename to handle potential leading './' in archive paths\n",
    "                        member_basename = os.path.basename(member.name)\n",
    "                        if (\n",
    "                            member_basename in correct_test_files and member.isfile()\n",
    "                        ):  # Also ensure it's a file\n",
    "                            # Extract it directly into the data_root_dir, overwriting if necessary\n",
    "                            member.name = member_basename  # Ensure it extracts flatly in data_root_dir\n",
    "                            tar.extract(member, path=data_root_dir)\n",
    "                            print(f\"  Extracted: {member.name}\")\n",
    "                            extracted_count += 1\n",
    "                    if extracted_count == len(correct_test_files):\n",
    "                        print(\n",
    "                            f\"Successfully extracted {extracted_count} required test files.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Warning: Expected {len(correct_test_files)} test files, but extracted {extracted_count}. Check archive content.\"\n",
    "                        )\n",
    "            except tarfile.TarError as e:\n",
    "                print(\n",
    "                    f\"Error extracting {test_archive_path}: {e}. Manual cleanup might be needed.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred during manual extraction: {e}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Test archive {test_archive_path} not found after download attempt. Cannot manually extract.\"\n",
    "            )\n",
    "        # --- End manual extraction ---\n",
    "\n",
    "        print(\"Loading spaCy models...\")\n",
    "        self._load_spacy_models()  # Ensure models are available\n",
    "        print(\"prepare_data() complete.\")\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        \"\"\"Build vocabularies and process datasets. Runs on each process.\"\"\"\n",
    "        # Load iterators\n",
    "        train_iter, valid_iter, test_iter = Multi30k(\n",
    "            root=self.data_dir,\n",
    "            split=(\"train\", \"valid\", \"test\"),\n",
    "            language_pair=(\"de\", \"en\"),\n",
    "        )\n",
    "        # Load train_iter fully for vocab building (might be memory intensive for huge datasets)\n",
    "        train_list = list(train_iter)\n",
    "\n",
    "        # Build vocabularies only if they haven't been built\n",
    "        # Ensure tokenizers are ready before building vocab\n",
    "        if self.spacy_de is None or self.spacy_en is None:\n",
    "            self._load_spacy_models()\n",
    "\n",
    "        if self.de_vocab is None:\n",
    "            print(\"Building German vocabulary...\")\n",
    "            self.de_vocab = build_vocab_from_iterator(\n",
    "                self._yield_tokens((src for src, _ in train_list), self.tokenize_de),\n",
    "                min_freq=2,  # Ignore rare words\n",
    "                specials=SPECIAL_TOKENS,\n",
    "                special_first=True,\n",
    "            )\n",
    "            self.de_vocab.set_default_index(\n",
    "                UNK_IDX\n",
    "            )  # Set default for out-of-vocabulary words\n",
    "\n",
    "        if self.en_vocab is None:\n",
    "            print(\"Building English vocabulary...\")\n",
    "            self.en_vocab = build_vocab_from_iterator(\n",
    "                self._yield_tokens((tgt for _, tgt in train_list), self.tokenize_en),\n",
    "                min_freq=2,\n",
    "                specials=SPECIAL_TOKENS,\n",
    "                special_first=True,\n",
    "            )\n",
    "            self.en_vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "        print(\"Processing datasets...\")\n",
    "        # Process data (convert text to indices)\n",
    "        train_src, train_tgt = self._process_data(train_list)\n",
    "        valid_src, valid_tgt = self._process_data(list(valid_iter))\n",
    "        print(\"Attempting to process test_iter...\")  # Log before processing test_iter\n",
    "        test_src, test_tgt = self._process_data(list(test_iter))\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_data = TranslationDataset(train_src, train_tgt)\n",
    "        self.valid_data = TranslationDataset(valid_src, valid_tgt)\n",
    "        self.test_data = TranslationDataset(test_src, test_tgt)\n",
    "        print(\"Data setup complete.\")\n",
    "\n",
    "    def _process_data(self, data):\n",
    "        \"\"\"Process raw data pairs into tokenized and indexed tensors.\"\"\"\n",
    "        src_tensors = []\n",
    "        tgt_tensors = []\n",
    "        for src_text, tgt_text in data:\n",
    "            src_tokens = self.tokenize_de(src_text)\n",
    "            tgt_tokens = self.tokenize_en(tgt_text)\n",
    "\n",
    "            src_indices = [self.de_vocab[token] for token in src_tokens]\n",
    "            # Add SOS/EOS to target sequence only\n",
    "            tgt_indices = (\n",
    "                [SOS_IDX] + [self.en_vocab[token] for token in tgt_tokens] + [EOS_IDX]\n",
    "            )\n",
    "\n",
    "            src_tensors.append(torch.tensor(src_indices, dtype=torch.long))\n",
    "            tgt_tensors.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
    "        return src_tensors, tgt_tensors\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Collate function to pad sequences within a batch.\"\"\"\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for src_sample, tgt_sample in batch:  # Corrected variable names\n",
    "            src_batch.append(src_sample)\n",
    "            tgt_batch.append(tgt_sample)\n",
    "\n",
    "        # Pad sequences to the max length in the current batch\n",
    "        src_batch = pad_sequence(\n",
    "            src_batch, padding_value=self.de_vocab[PAD_TOKEN], batch_first=True\n",
    "        )\n",
    "        tgt_batch = pad_sequence(\n",
    "            tgt_batch, padding_value=self.en_vocab[PAD_TOKEN], batch_first=True\n",
    "        )\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        if self.train_data is None:\n",
    "            self.setup()  # Ensure setup has run\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=True,  # May speed up GPU training\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        if self.valid_data is None:\n",
    "            self.setup()\n",
    "        return DataLoader(\n",
    "            self.valid_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the test set.\"\"\"\n",
    "        if self.test_data is None:\n",
    "            self.setup()\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450c6a3",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Model with PyTorch Lightning Module\n",
    "\n",
    "### `TransformerLightningModule`\n",
    "This class defines the core Transformer model logic within the PyTorch Lightning framework.\n",
    "- **Initialization**:\n",
    "    - Takes vocabulary sizes, model hyperparameters (d_model, heads, layers, d_ff, dropout, etc.), learning rate, and special token indices.\n",
    "    - Initializes the `Transformer` model (imported from `transformer_from_scratch`).\n",
    "    - Initializes the loss function (`CrossEntropyLoss`), ignoring the padding index.\n",
    "    - Initializes `BLEUScore` metrics for validation and testing.\n",
    "    - Stores references to vocabularies and tokenizers needed for BLEU calculation and translation.\n",
    "    - Uses `save_hyperparameters` to automatically log hyperparameters.\n",
    "- **`forward`**: Defines the forward pass, simply calling the internal `transformer` model. The `transformer`'s forward method handles mask creation internally.\n",
    "- **`_calculate_loss`**: A helper method to compute and log the loss for any step (train, val, test). It correctly prepares the target sequences for input (`tgt_input`) and loss calculation (`tgt_output_expected`) based on the standard teacher-forcing approach.\n",
    "- **`training_step`, `validation_step`, `test_step`**: Implement the standard Lightning steps, calling `_calculate_loss`. The validation and test steps also call `_update_bleu` to accumulate predictions and targets for BLEU score calculation.\n",
    "- **`_update_bleu`**: Calculates predicted sentences (using greedy decoding for simplicity during evaluation steps) and target sentences from a batch, converting indices back to text and handling special tokens. Updates the corresponding BLEU metric (`val_bleu` or `test_bleu`).\n",
    "- **`on_validation_epoch_end`, `on_test_epoch_end`**: Compute the final BLEU score for the epoch using the accumulated metric state, log it, and reset the metric for the next epoch/run.\n",
    "- **`configure_optimizers`**: Sets up the Adam optimizer and a `ReduceLROnPlateau` learning rate scheduler that reduces the learning rate if the validation loss plateaus.\n",
    "- **`translate_sentence`**: Provides a method to translate a single German sentence into English using the trained model. It handles tokenization, indexing, running the model autoregressively (feeding predictions back as input), and converting the output indices back to a readable sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ebee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_from_scratch import Transformer\n",
    "\n",
    "\n",
    "class TransformerLightningModule(pl.LightningModule):\n",
    "    \"\"\"LightningModule for the Transformer model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 3,\n",
    "        d_ff: int = 512,\n",
    "        max_seq_length: int = 100,\n",
    "        dropout: float = 0.1,\n",
    "        learning_rate: float = 0.0005,\n",
    "        pad_idx: int = PAD_IDX,\n",
    "        sos_idx: int = SOS_IDX,\n",
    "        eos_idx: int = EOS_IDX,\n",
    "        de_vocab=None,  # Pass vocab/tokenizer for translation & BLEU\n",
    "        en_vocab=None,\n",
    "        tokenize_de=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Store hyperparameters automatically, ignore non-primitive types for logging\n",
    "        self.save_hyperparameters(ignore=[\"de_vocab\", \"en_vocab\", \"tokenize_de\"])\n",
    "\n",
    "        # Ensure pad_idx is passed correctly if needed by Transformer sub-modules\n",
    "        # Assuming Transformer internal components get pad_idx from somewhere else or don't need it explicitly in init\n",
    "        self.transformer = Transformer(\n",
    "            src_vocab_size=src_vocab_size,\n",
    "            tgt_vocab_size=tgt_vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_ff=d_ff,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.hparams.pad_idx)\n",
    "        # Access learning rate via self.hparams\n",
    "        # self.learning_rate = learning_rate (already saved in hparams)\n",
    "\n",
    "        # Store necessary components passed for BLEU/translation\n",
    "        # Using _ prefix to indicate they are primarily for internal use within the module\n",
    "        self._de_vocab = de_vocab\n",
    "        self._en_vocab = en_vocab\n",
    "        self._tokenize_de = tokenize_de\n",
    "\n",
    "        # Initialize BLEU score metrics\n",
    "        self.val_bleu = BLEUScore()\n",
    "        self.test_bleu = BLEUScore()\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Forward pass through the Transformer model.\"\"\"\n",
    "        return self.transformer(src, tgt)\n",
    "    \n",
    "    def _calculate_loss(self, batch, stage=\"train\"):\n",
    "        \"\"\"Helper function to calculate and log loss.\"\"\"\n",
    "        src, tgt = batch\n",
    "        # Prepare target sequences:\n",
    "        # Input to decoder: Starts with SOS, ends before EOS\n",
    "        # Target for loss: Starts after SOS, ends with EOS\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output_expected = tgt[:, 1:]\n",
    "\n",
    "        # Get model prediction\n",
    "        # The model's forward takes src and decoder input (tgt_input)\n",
    "        output = self(\n",
    "            src, tgt_input\n",
    "        )  # Shape: (batch_size, tgt_len - 1, tgt_vocab_size)\n",
    "\n",
    "        # Reshape for CrossEntropyLoss (expects N, C)\n",
    "        # N = batch_size * sequence_length, C = vocab_size\n",
    "        output_dim = output.shape[-1]\n",
    "        output_flat = output.contiguous().view(\n",
    "            -1, output_dim\n",
    "        )  # Shape: (batch_size * (tgt_len - 1), tgt_vocab_size)\n",
    "        tgt_flat = tgt_output_expected.contiguous().view(\n",
    "            -1\n",
    "        )  # Shape: (batch_size * (tgt_len - 1))\n",
    "\n",
    "        loss = self.criterion(output_flat, tgt_flat)\n",
    "\n",
    "        # Log the loss\n",
    "        self.log(\n",
    "            f\"{stage}_loss\",\n",
    "            loss,\n",
    "            on_step=(stage == \"train\"),\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss, output  # Return raw output for BLEU calculation\n",
    "    \n",
    "    def _update_bleu(self, batch, model_output, bleu_metric):\n",
    "        \"\"\"Helper to calculate and update BLEU score for a batch.\"\"\"\n",
    "        if self._en_vocab is None:\n",
    "            return  # Cannot calculate BLEU without vocab\n",
    "\n",
    "        src, tgt = batch\n",
    "        tgt_output_expected = tgt[:, 1:]  # Target for comparison (batch_size, seq_len)\n",
    "\n",
    "        # Get predicted token indices (greedy decoding)\n",
    "        # model_output shape: (batch_size, seq_len, vocab_size)\n",
    "        pred_indices = model_output.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Convert indices to text sentences\n",
    "        pred_sentences_text = []\n",
    "        target_sentences_text = []\n",
    "\n",
    "        itos = self._en_vocab.get_itos()  # Integer-to-string mapping\n",
    "\n",
    "        for i in range(pred_indices.shape[0]):  # Iterate through batch\n",
    "            # Predicted sentence\n",
    "            pred_sent = []\n",
    "            for idx in pred_indices[i].tolist():\n",
    "                if idx == self.hparams.eos_idx:\n",
    "                    break\n",
    "                # Include UNK, but skip PAD and SOS\n",
    "                if idx != self.hparams.pad_idx and idx != self.hparams.sos_idx:\n",
    "                    pred_sent.append(itos[idx])\n",
    "            pred_sentences_text.append(\" \".join(pred_sent))\n",
    "\n",
    "            # Target sentence (needs to be list of lists for BLEU score)\n",
    "            target_sent = []\n",
    "            for idx in tgt_output_expected[i].tolist():\n",
    "                if idx == self.hparams.eos_idx:\n",
    "                    break\n",
    "                # Skip PAD and SOS\n",
    "                if idx != self.hparams.pad_idx and idx != self.hparams.sos_idx:\n",
    "                    target_sent.append(itos[idx])\n",
    "            # BLEU score expects list of reference sentences\n",
    "            target_sentences_text.append([\" \".join(target_sent)])\n",
    "\n",
    "        # Update the BLEU metric state\n",
    "        if pred_sentences_text and target_sentences_text:  # Ensure not empty\n",
    "            bleu_metric.update(pred_sentences_text, target_sentences_text)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Performs a single training step.\"\"\"\n",
    "        loss, _ = self._calculate_loss(batch, stage=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Performs a single validation step.\"\"\"\n",
    "        loss, output = self._calculate_loss(batch, stage=\"val\")\n",
    "        # Calculate BLEU score for this batch\n",
    "        self._update_bleu(batch, output, self.val_bleu)\n",
    "        # Loss is automatically logged by _calculate_loss\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Performs a single test step.\"\"\"\n",
    "        loss, output = self._calculate_loss(batch, stage=\"test\")\n",
    "        # Calculate BLEU score for this batch\n",
    "        self._update_bleu(batch, output, self.test_bleu)\n",
    "        # Loss is automatically logged by _calculate_loss\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Computes and logs validation BLEU score at the end of the epoch.\"\"\"\n",
    "        if self._en_vocab:  # Only compute if vocab is available\n",
    "            bleu = self.val_bleu.compute()\n",
    "            self.log(\"val_bleu\", bleu, prog_bar=True)\n",
    "            self.val_bleu.reset()  # Reset metric state for next epoch\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Computes and logs test BLEU score at the end of the test run.\"\"\"\n",
    "        if self._en_vocab:\n",
    "            bleu = self.test_bleu.compute()\n",
    "            self.log(\"test_bleu\", bleu, prog_bar=True)\n",
    "            self.test_bleu.reset()  # Reset metric state\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configures the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        # Learning rate scheduler: Reduces LR when validation loss plateaus\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",  # Reduce LR when the monitored quantity stops decreasing\n",
    "            factor=0.1,  # Factor by which the learning rate will be reduced\n",
    "            patience=2,  # Number of epochs with no improvement after which LR will be reduced\n",
    "            verbose=True,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",  # Quantity to monitor\n",
    "                \"interval\": \"epoch\",  # Check scheduler condition every epoch\n",
    "                \"frequency\": 1,  # Check every 1 epoch\n",
    "            },\n",
    "        }\n",
    "        \n",
    "    def translate_sentence(self, sentence: str, max_len=50):\n",
    "        \"\"\"Translates a single source sentence using the trained model.\"\"\"\n",
    "        # Ensure necessary components are available\n",
    "        if (\n",
    "            self._de_vocab is None\n",
    "            or self._en_vocab is None\n",
    "            or self._tokenize_de is None\n",
    "        ):\n",
    "            raise RuntimeError(\n",
    "                \"Vocabularies and tokenizer must be available for translation. Ensure they are passed during init or loaded.\"\n",
    "            )\n",
    "\n",
    "        self.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "        # Tokenize and numericalize the source sentence\n",
    "        tokens = self._tokenize_de(sentence)\n",
    "        indices = [self._de_vocab[token] for token in tokens]\n",
    "        # Add batch dimension and move to the correct device\n",
    "        src_tensor = torch.LongTensor(indices).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Initialize the target sequence with the SOS token\n",
    "        tgt_tensor = torch.LongTensor([[self.hparams.sos_idx]]).to(\n",
    "            self.device\n",
    "        )  # Shape: (1, 1)\n",
    "\n",
    "        # Autoregressive generation loop\n",
    "        for _ in range(max_len):\n",
    "            with torch.no_grad():  # No need to track gradients during inference\n",
    "                # Pass current source and target sequence to the model\n",
    "                # The model's forward pass will handle mask creation\n",
    "                output = self.transformer(\n",
    "                    src_tensor, tgt_tensor\n",
    "                )  # Output shape: (1, current_tgt_len, tgt_vocab_size)\n",
    "\n",
    "            # Get the predicted token index for the *last* position in the sequence\n",
    "            pred_token_idx = output.argmax(2)[:, -1].item()\n",
    "\n",
    "            # Append the predicted token index to the target sequence\n",
    "            # Shape becomes (1, current_tgt_len + 1)\n",
    "            tgt_tensor = torch.cat(\n",
    "                [tgt_tensor, torch.LongTensor([[pred_token_idx]]).to(self.device)],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            # Stop generation if the EOS token is predicted\n",
    "            if pred_token_idx == self.hparams.eos_idx:\n",
    "                break\n",
    "\n",
    "        # Convert the generated target indices back to tokens\n",
    "        # Skip the initial SOS token ([1:])\n",
    "        tgt_indices = tgt_tensor.squeeze(0).tolist()[1:]\n",
    "        tgt_tokens = []\n",
    "        itos = self._en_vocab.get_itos()\n",
    "        for idx in tgt_indices:\n",
    "            if idx == self.hparams.eos_idx:  # Stop if EOS is encountered\n",
    "                break\n",
    "            tgt_tokens.append(itos[idx])\n",
    "\n",
    "        return \" \".join(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb4d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 64,  # Adjusted batch size\n",
    "    \"d_model\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 3,\n",
    "    \"d_ff\": 512,\n",
    "    \"max_seq_length\": 100,  # Should match model capacity\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"num_epochs\": 30,\n",
    "    \"patience\": 5,  # Early stopping patience\n",
    "    \"data_dir\": \".data\",  # Directory for dataset download\n",
    "    \"checkpoint_dir\": \"checkpoints_transformer\",  # Directory for saving models\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f44b8",
   "metadata": {},
   "source": [
    "# üß† Gi·∫£i th√≠ch c·∫•u h√¨nh `config = {...}` t·ª´ng d√≤ng\n",
    "\n",
    "| Tham s·ªë               | Gi√° tr·ªã                         | Gi·∫£i th√≠ch                                                                 |\n",
    "|-----------------------|----------------------------------|-----------------------------------------------------------------------------|\n",
    "| `batch_size`          | 64                               | M·ªói l·∫ßn hu·∫•n luy·ªán, m√¥ h√¨nh x·ª≠ l√Ω 64 c·∫∑p c√¢u (ngu·ªìn + ƒë√≠ch).              |\n",
    "| `d_model`             | 256                              | M·ªói t·ª´ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng vector 256 chi·ªÅu (embedding + transformer layers). |\n",
    "| `num_heads`           | 8                                | Multi-head attention c√≥ 8 ƒë·∫ßu ƒë·ªÉ h·ªçc c√°c quan h·ªá kh√°c nhau gi·ªØa c√°c t·ª´.   |\n",
    "| `num_layers`          | 3                                | G·ªìm 3 encoder layer + 3 decoder layer.                                     |\n",
    "| `d_ff`                | 512                              | Feed-forward network g·ªìm 2 l·ªõp, m·ªói l·ªõp c√≥ 512 neuron.                     |\n",
    "| `max_seq_length`      | 100                              | Chi·ªÅu d√†i t·ªëi ƒëa c·ªßa c√¢u. N·∫øu ng·∫Øn h∆°n th√¨ padding, d√†i h∆°n th√¨ b·ªã c·∫Øt.    |\n",
    "| `dropout`             | 0.1                              | Lo·∫°i ng·∫´u nhi√™n 10% neuron ƒë·ªÉ tr√°nh overfitting trong l√∫c hu·∫•n luy·ªán.     |\n",
    "| `learning_rate`       | 0.0005                           | T·ªëc ƒë·ªô h·ªçc. B∆∞·ªõc nh·∫£y nh·ªè gi√∫p m√¥ h√¨nh h·ªçc ch·∫≠m m√† ch·∫Øc.                  |\n",
    "| `num_epochs`          | 30                               | S·ªë l·∫ßn h·ªçc qua to√†n b·ªô d·ªØ li·ªáu. T·ªëi ƒëa 30 l·∫ßn.                             |\n",
    "| `patience`            | 5                                | Early stopping. N·∫øu validation kh√¥ng c·∫£i thi·ªán sau 5 epoch th√¨ d·ª´ng s·ªõm.  |\n",
    "| `data_dir`            | \".data\"                          | Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu d·ªãch nh∆∞ IWSLT ho·∫∑c Multi30K.                         |\n",
    "| `checkpoint_dir`      | \"checkpoints_transformer\"        | Th∆∞ m·ª•c l∆∞u m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n `val_loss`.                         |\n",
    "\n",
    "# üì¶ V√≠ d·ª• minh h·ªça\n",
    "\n",
    "Gi·∫£ s·ª≠ b·∫°n ƒëang d·ªãch:\n",
    "\n",
    "> \"Ich liebe dich\" ‚Üí \"I love you\"\n",
    "\n",
    "Th√¨ c√°c th√¥ng s·ªë ·∫£nh h∆∞·ªüng nh∆∞ sau:\n",
    "\n",
    "* `d_model`: 256 ‚Üí \"Ich\" ƒë∆∞·ª£c chuy·ªÉn th√†nh vector 256 chi·ªÅu  \n",
    "* `num_heads`: 8 ‚Üí M√¥ h√¨nh c√≥ 8 g√≥c nh√¨n ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa \"liebe\", \"Ich\", \"dich\"  \n",
    "* `max_seq_length`: 100 ‚Üí N·∫øu c√¢u d√†i h∆°n 100 t·ª´ th√¨ s·∫Ω b·ªã c·∫Øt  \n",
    "* `dropout`: 0.1 ‚Üí Ng·∫´u nhi√™n t·∫Øt b·ªõt neuron ƒë·ªÉ tr√°nh h·ªçc l·ªách  \n",
    "* `learning_rate`: 0.0005 ‚Üí M√¥ h√¨nh h·ªçc t·ª´ t·ª´ ƒë·ªÉ t·ªëi ∆∞u ch√≠nh x√°c\n",
    "\n",
    "# üí° G·ª£i √Ω c·∫•u h√¨nh m·ªü r·ªông (tu·ª≥ ch·ªçn)\n",
    "\n",
    "* `label_smoothing`: 0.1  \n",
    "  ‚Üí Gi·∫£m ƒë·ªô t·ª± tin qu√° m·ª©c khi training v·ªõi CrossEntropyLoss\n",
    "\n",
    "* `warmup_steps`: 4000  \n",
    "  ‚Üí D√πng khi √°p d·ª•ng Noam Scheduler t·ª´ paper g·ªëc\n",
    "\n",
    "* `gradient_clip_val`: 1.0  \n",
    "  ‚Üí Tr√°nh hi·ªán t∆∞·ª£ng gradient exploding khi hu·∫•n luy·ªán\n",
    "\n",
    "* `save_top_k`: 1  \n",
    "  ‚Üí Ch·ªâ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t\n",
    "\n",
    "* `monitor`: \"val_loss\"  \n",
    "  ‚Üí D·ª±a v√†o ch·ªâ s·ªë n√†y ƒë·ªÉ quy·∫øt ƒë·ªãnh khi n√†o l∆∞u m√¥ h√¨nh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1ff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = TransformerDataModule(\n",
    "    batch_size=config[\"batch_size\"], data_dir=config[\"data_dir\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbaed65",
   "metadata": {},
   "source": [
    "# üß† Gi·∫£i th√≠ch t·ª´ng d√≤ng d√†nh cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu\n",
    "\n",
    "## 1. `TransformerDataModule(...)`\n",
    "\n",
    "| Th√†nh ph·∫ßn                    | M√¥ t·∫£ chi ti·∫øt                                                                 |\n",
    "|------------------------------|----------------------------------------------------------------------------------|\n",
    "| **L√† g√¨?**                   | M·ªôt **DataModule** theo chu·∫©n c·ªßa **PyTorch Lightning**                         |\n",
    "| **Ch·ª©c nƒÉng**                | - Qu·∫£n l√Ω vi·ªác t·∫£i, x·ª≠ l√Ω, chia t·∫≠p `train`, `val`, `test` t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o   |\n",
    "|                              | - T·∫°o ra c√°c `DataLoader` chu·∫©n ƒë·ªÉ ph·ª•c v·ª• m√¥ h√¨nh Transformer                  |\n",
    "| **∆Øu ƒëi·ªÉm**                  | ƒê·ªãnh nghƒ©a m·ªôt l·∫ßn duy nh·∫•t, sau ƒë√≥ s·ª≠ d·ª•ng l·∫°i ·ªü b·∫•t k·ª≥ giai ƒëo·∫°n n√†o          |\n",
    "|                              | nh∆∞ hu·∫•n luy·ªán, ki·ªÉm th·ª≠, validate, ho·∫∑c d·ªãch c√¢u                                |\n",
    "| **T√≠ch h·ª£p**                 | T∆∞∆°ng th√≠ch v·ªõi Lightning, gi√∫p qu·∫£n l√Ω d·ªØ li·ªáu xuy√™n su·ªët pipeline hu·∫•n luy·ªán  |\n",
    "\n",
    "üéØ **M·ª•c ti√™u:**  \n",
    "Gi√∫p b·∫°n kh√¥ng ph·∫£i vi·∫øt l·∫°i code x·ª≠ l√Ω d·ªØ li·ªáu cho t·ª´ng giai ƒëo·∫°n. Ch·ªâ c·∫ßn kh·ªüi t·∫°o 1 l·∫ßn l√† d√πng ƒë∆∞·ª£c m·ªçi n∆°i.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üî¢ Tham s·ªë truy·ªÅn v√†o\n",
    "\n",
    "| Tham s·ªë       | √ù nghƒ©a                                                                                        |\n",
    "|---------------|------------------------------------------------------------------------------------------------|\n",
    "| `batch_size`  | S·ªë l∆∞·ª£ng c·∫∑p c√¢u (ngu·ªìn + ƒë√≠ch) ƒë∆∞·ª£c x·ª≠ l√Ω m·ªói l·∫ßn training ho·∫∑c validation. L·∫•y t·ª´ `config`. |\n",
    "| `data_dir`    | Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu d·ªãch: bao g·ªìm file `raw`, tokenizer, vocabulary‚Ä¶                         |\n",
    "\n",
    "üí° **M·ª•c ƒë√≠ch:**  \n",
    "Gi√∫p m√¥ h√¨nh t·ª± ƒë·ªông qu·∫£n l√Ω m·ªçi th·ª© li√™n quan ƒë·∫øn d·ªØ li·ªáu ƒë·∫ßu v√†o, b·∫°n kh√¥ng c·∫ßn truy·ªÅn th·ªß c√¥ng cho t·ª´ng `DataLoader`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with Multi30k download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually extracting required test files from .data/datasets/Multi30k/mmt16_task1_test.tar.gz...\n",
      "  Found member in archive: ./._test.de (Is file: True)\n",
      "  Found member in archive: ./test.de (Is file: True)\n",
      "  Extracted: test.de\n",
      "  Found member in archive: ./._test.en (Is file: True)\n",
      "  Found member in archive: ./test.en (Is file: True)\n",
      "  Extracted: test.en\n",
      "  Found member in archive: ./._test.fr (Is file: True)\n",
      "  Found member in archive: ./test.fr (Is file: True)\n",
      "Successfully extracted 2 required test files.\n",
      "Loading spaCy models...\n",
      "prepare_data() complete.\n"
     ]
    }
   ],
   "source": [
    "# We prepare the data module to ensure the vocabularies are built\n",
    "datamodule.prepare_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488b43b",
   "metadata": {},
   "source": [
    "# üß† Gi·∫£i th√≠ch d√≤ng `datamodule.prepare_data()`\n",
    "\n",
    "| Th√†nh ph·∫ßn                 | Gi·∫£i th√≠ch chi ti·∫øt                                                                 |\n",
    "|---------------------------|--------------------------------------------------------------------------------------|\n",
    "| **M·ª•c ti√™u ch√≠nh**        | Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho m√¥ h√¨nh d·ªãch b·∫±ng Transformer                          |\n",
    "| **T·∫£i d·ªØ li·ªáu**           | T·∫£i + gi·∫£i n√©n dataset nh∆∞ Multi30k, IWSLT n·∫øu ch∆∞a c√≥ trong th∆∞ m·ª•c                |\n",
    "| **Ti·ªÅn x·ª≠ l√Ω c√¢u**        | Chu·∫©n h√≥a, chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng (`lowercase`), lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát, l√†m s·∫°ch c√¢u    |\n",
    "| **Tokenizer**             | T√°ch c√¢u th√†nh t·ª´ng token ho·∫∑c subword (d√πng Spacy, BPE, SentencePiece...)          |\n",
    "| **T·∫°o vocabulary**        | √Ånh x·∫° m·ªói t·ª´ v·ªÅ m·ªôt ch·ªâ s·ªë (`\"hello\"` ‚Üí 482, `\"world\"` ‚Üí 203)                      |\n",
    "| **L∆∞u vocab ra file**     | L∆∞u file vocab ƒë·ªÉ t√°i s·ª≠ d·ª•ng l·∫ßn sau, tr√°nh vi·ªác t·∫°o l·∫°i m·ªói l·∫ßn ch·∫°y              |\n",
    "| **R√†ng bu·ªôc b·∫Øt bu·ªôc**    | N·∫øu kh√¥ng g·ªçi `prepare_data()`, khi hu·∫•n luy·ªán s·∫Ω l·ªói: `\"vocab is None\"`            |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ T√≥m t·∫Øt l·ª£i √≠ch\n",
    "\n",
    "* Gi√∫p x·ª≠ l√Ω v√† chu·∫©n h√≥a to√†n b·ªô d·ªØ li·ªáu ng√¥n ng·ªØ.\n",
    "* T·∫°o token v√† vocab cho m√¥ h√¨nh s·ª≠ d·ª•ng sau n√†y.\n",
    "* L√† b∆∞·ªõc b·∫Øt bu·ªôc ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ hu·∫•n luy·ªán, d·ª± ƒëo√°n, ho·∫∑c ƒë√°nh gi√°.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° S∆° ƒë·ªì minh h·ªça tr·ª±c quan\n",
    "\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ Raw Data (en-de)   ‚îÇ\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ prepare_data()       ‚îÇ\n",
    "        ‚îÇ                      ‚îÇ\n",
    "        ‚îÇ  ‚úî Load + clean data ‚îÇ\n",
    "        ‚îÇ  ‚úî Tokenize          ‚îÇ\n",
    "        ‚îÇ  ‚úî Build vocab       ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚ñº\n",
    "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "      ‚îÇ  self.de_vocab, en_vocab  ‚îÇ\n",
    "      ‚îÇ  self.train_dataloader()  ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb53da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building German vocabulary...\n",
      "Building English vocabulary...\n",
      "Processing datasets...\n",
      "Attempting to process test_iter...\n",
      "Data setup complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup the model so that the vocabularies are built\n",
    "datamodule.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f9d1",
   "metadata": {},
   "source": [
    "# ‚úÖ Gi·∫£i th√≠ch chi ti·∫øt cho ng∆∞·ªùi m·ªõi h·ªçc\n",
    "\n",
    "## 1. G·ªçi `datamodule.prepare_data()`\n",
    "\n",
    "* D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c t·∫£i v·ªÅ (Multi30k, IWSLT, ...).\n",
    "* Th·ª±c hi·ªán x·ª≠ l√Ω th√¥: chu·∫©n h√≥a, lowercase, lo·∫°i b·ªè k√Ω t·ª± l·∫°.\n",
    "* Tokenize c√°c c√¢u b·∫±ng tokenizer (Spacy, BPE...).\n",
    "* T·∫°o vocabulary: √°nh x·∫° m·ªói token ‚Üí ch·ªâ s·ªë.\n",
    "  - V√≠ d·ª•: `\"hello\"` ‚Üí `482`, `\"world\"` ‚Üí `203`\n",
    "* L∆∞u l·∫°i c√°c file vocab ƒë·ªÉ d√πng l·∫°i sau.\n",
    "* ‚ö†Ô∏è N·∫øu b·ªè qua b∆∞·ªõc n√†y ‚Üí l·ªói `\"vocab is None\"` khi hu·∫•n luy·ªán ho·∫∑c d·ªãch c√¢u.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. G·ªçi `datamodule.setup()`\n",
    "\n",
    "* Chia d·ªØ li·ªáu ra th√†nh: `train`, `val`, `test`.\n",
    "* L∆∞u c√°c vocab ƒë√£ t·∫°o v√†o bi·∫øn n·ªôi b·ªô nh∆∞:\n",
    "  - `datamodule.de_vocab`\n",
    "  - `datamodule.en_vocab`\n",
    "* T·∫°o s·∫µn c√°c DataLoader:\n",
    "  - `train_dataloader()`\n",
    "  - `val_dataloader()`\n",
    "  - `test_dataloader()`\n",
    "\n",
    "üéØ **M·ª•c ƒë√≠ch:** chu·∫©n b·ªã m·ªçi th·ª© ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ hu·∫•n luy·ªán ho·∫∑c d·ªãch c√¢u.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ So s√°nh `prepare_data()` v√† `setup()`\n",
    "\n",
    "| Giai ƒëo·∫°n        | Ch·ª©c nƒÉng ch√≠nh                                      |\n",
    "|------------------|------------------------------------------------------|\n",
    "| `prepare_data()` | T·∫£i d·ªØ li·ªáu + ti·ªÅn x·ª≠ l√Ω + t·∫°o tokenizer/vocab       |\n",
    "| `setup()`        | Chia th√†nh c√°c t·∫≠p `train/val/test` + chu·∫©n b·ªã DataLoader |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© S∆° ƒë·ªì minh h·ªça lu·ªìng d·ªØ li·ªáu\n",
    "\n",
    "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "               ‚îÇ  prepare_data()      ‚îÇ\n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚ñº\n",
    "             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "             ‚îÇ Tokenizer + Vocabulary    ‚îÇ\n",
    "             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ   setup()          ‚îÇ\n",
    "              ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "              ‚îÇ ‚îÇ Train Dataset ‚îÇ ‚îÇ\n",
    "              ‚îÇ ‚îÇ Val Dataset   ‚îÇ ‚îÇ\n",
    "              ‚îÇ ‚îÇ Test Dataset  ‚îÇ ‚îÇ\n",
    "              ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ DataLoaders Ready   ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "---\n",
    "\n",
    "## üîö Sau setup, b·∫°n c√≥ th·ªÉ:\n",
    "\n",
    "* ‚úÖ D·ªãch c√¢u:\n",
    "\n",
    "model._de_vocab = datamodule.de_vocab\n",
    "model._en_vocab = datamodule.en_vocab\n",
    "model._tokenize_de = datamodule.tokenize_de\n",
    "\n",
    "* ‚úÖ Hu·∫•n luy·ªán m√¥ h√¨nh:\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1823af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 7853\n",
      "Target vocabulary size: 5893\n",
      "PAD index: 0, SOS index: 1, EOS index: 2\n"
     ]
    }
   ],
   "source": [
    "# Get vocab sizes and special indices from the datamodule after setup\n",
    "src_vocab_size = len(datamodule.de_vocab)\n",
    "tgt_vocab_size = len(datamodule.en_vocab)\n",
    "pad_idx = datamodule.de_vocab[PAD_TOKEN]\n",
    "sos_idx = datamodule.en_vocab[SOS_TOKEN]\n",
    "eos_idx = datamodule.en_vocab[EOS_TOKEN]\n",
    "\n",
    "print(f\"Source vocabulary size: {src_vocab_size}\")\n",
    "print(f\"Target vocabulary size: {tgt_vocab_size}\")\n",
    "print(f\"PAD index: {pad_idx}, SOS index: {sos_idx}, EOS index: {eos_idx}\")\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbd8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Setup the model\n",
    "model = TransformerLightningModule(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=config[\"d_model\"],\n",
    "    num_heads=config[\"num_heads\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    d_ff=config[\"d_ff\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    dropout=config[\"dropout\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    pad_idx=pad_idx,  # Pass the actual PAD index\n",
    "    sos_idx=sos_idx,  # Pass the actual SOS index\n",
    "    eos_idx=eos_idx,  # Pass the actual EOS index\n",
    "    # Pass vocabs/tokenizer for BLEU calculation and translation method\n",
    "    de_vocab=datamodule.de_vocab,\n",
    "    en_vocab=datamodule.en_vocab,\n",
    "    tokenize_de=datamodule.tokenize_de,\n",
    ")\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8479163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Checkpoint callback: Saves the best model based on validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Metric to monitor\n",
    "    dirpath=config[\"checkpoint_dir\"],  # Directory to save checkpoints\n",
    "    filename=\"transformer-best-{epoch:02d}-{val_loss:.2f}-{val_bleu:.4f}\",  # Filename format\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode=\"min\",  # Mode for the monitored metric (minimize loss)\n",
    "    save_last=True,  # Optionally save the last checkpoint as well\n",
    ")\n",
    "# Early stopping callback: Stops training if validation loss doesn't improve\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Metric to monitor\n",
    "    patience=config[\"patience\"],  # Number of epochs to wait for improvement\n",
    "    verbose=True,  # Print messages when stopping\n",
    "    mode=\"min\",  # Mode for the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da189f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Proceeding with Multi30k download...\n",
      "Manually extracting required test files from .data/datasets/Multi30k/mmt16_task1_test.tar.gz...\n",
      "  Found member in archive: ./._test.de (Is file: True)\n",
      "  Found member in archive: ./test.de (Is file: True)\n",
      "  Extracted: test.de\n",
      "  Found member in archive: ./._test.en (Is file: True)\n",
      "  Found member in archive: ./test.en (Is file: True)\n",
      "  Extracted: test.en\n",
      "  Found member in archive: ./._test.fr (Is file: True)\n",
      "  Found member in archive: ./test.fr (Is file: True)\n",
      "Successfully extracted 2 required test files.\n",
      "Loading spaCy models...\n",
      "prepare_data() complete.\n",
      "Processing datasets...\n",
      "Attempting to process test_iter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type             | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | transformer | Transformer      | 9.0 M  | train\n",
      "1 | criterion   | CrossEntropyLoss | 0      | train\n",
      "2 | _de_vocab   | Vocab            | 0      | train\n",
      "3 | _en_vocab   | Vocab            | 0      | train\n",
      "4 | val_bleu    | BLEUScore        | 0      | train\n",
      "5 | test_bleu   | BLEUScore        | 0      | train\n",
      "---------------------------------------------------------\n",
      "9.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.0 M     Total params\n",
      "35.949    Total estimated model params size (MB)\n",
      "134       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a079639ef58469887199537a56c2efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef867e15130c411191fcd91d49f52d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c52c5ce97ae4c59a26999552e8d5b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecc5b605fdf49c388d1048d693ada69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.459 >= min_delta = 0.0. New best score: 2.187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d50b5d4d2ef4c7989673bfc7ebde555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.234 >= min_delta = 0.0. New best score: 1.953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e43cc2bdb5491a8288e6bb74be1779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.107 >= min_delta = 0.0. New best score: 1.846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82361185767a4a1db1d5d485a241477c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.071 >= min_delta = 0.0. New best score: 1.775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54aec91109e4413bb0afd3f14cb059a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.048 >= min_delta = 0.0. New best score: 1.727\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:339\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_batch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m call._call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_batch_end\u001b[39m\u001b[33m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:227\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/tqdm_progress.py:279\u001b[39m, in \u001b[36mTQDMProgressBar.on_train_batch_end\u001b[39m\u001b[34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[39m\n\u001b[32m    278\u001b[39m _update_n(\u001b[38;5;28mself\u001b[39m.train_progress_bar, n)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28mself\u001b[39m.train_progress_bar.set_postfix(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/progress_bar.py:198\u001b[39m, in \u001b[36mProgressBar.get_metrics\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m    197\u001b[39m standard_metrics = get_standard_metrics(trainer)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m pbar_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n\u001b[32m    199\u001b[39m duplicates = \u001b[38;5;28mlist\u001b[39m(standard_metrics.keys() & pbar_metrics.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1667\u001b[39m, in \u001b[36mTrainer.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The metrics sent to the progress bar.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \n\u001b[32m   1663\u001b[39m \u001b[33;03mThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\u001b[39;00m\n\u001b[32m   1664\u001b[39m \u001b[33;03m:paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\u001b[39;00m\n\u001b[32m   1665\u001b[39m \n\u001b[32m   1666\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_logger_connector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:254\u001b[39m, in \u001b[36m_LoggerConnector.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._progress_bar_metrics.update(metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:235\u001b[39m, in \u001b[36m_LoggerConnector.metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:493\u001b[39m, in \u001b[36m_ResultCollection.metrics\u001b[39m\u001b[34m(self, on_step)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result_metric.meta.prog_bar:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m         metrics[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m][forked_name] = \u001b[43mconvert_tensors_to_scalars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:136\u001b[39m, in \u001b[36mconvert_tensors_to_scalars\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value.item()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_item\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py:66\u001b[39m, in \u001b[36mapply_to_collection\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:134\u001b[39m, in \u001b[36mconvert_tensors_to_scalars.<locals>.to_item\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    132\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe metric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` does not contain a single element, thus it cannot be converted to a scalar.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass both model and datamodule\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Initialize the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config[\"num_epochs\"],\n",
    "    accelerator=\"auto\",  # Automatically selects GPU/MPS/CPU\n",
    "    devices=-1,  # Use all devices (can be >1 for multi-GPU)\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],  # List of callbacks\n",
    "    log_every_n_steps=20,  # How often to log metrics\n",
    ")\n",
    "# Start the training process\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "trainer.fit(model, datamodule=datamodule)  # Pass both model and datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf84975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for testing from: /home/giangson/nlp/Chapter_06_Transformer_Architecture/checkpoints_transformer/transformer-best-epoch=05-val_loss=1.73-val_bleu=0.3321.ckpt\n",
      "Proceeding with Multi30k download...\n",
      "Manually extracting required test files from .data/datasets/Multi30k/mmt16_task1_test.tar.gz...\n",
      "  Found member in archive: ./._test.de (Is file: True)\n",
      "  Found member in archive: ./test.de (Is file: True)\n",
      "  Extracted: test.de\n",
      "  Found member in archive: ./._test.en (Is file: True)\n",
      "  Found member in archive: ./test.en (Is file: True)\n",
      "  Extracted: test.en\n",
      "  Found member in archive: ./._test.fr (Is file: True)\n",
      "  Found member in archive: ./test.fr (Is file: True)\n",
      "Successfully extracted 2 required test files.\n",
      "Loading spaCy models...\n",
      "prepare_data() complete.\n",
      "Processing datasets...\n",
      "Attempting to process test_iter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/giangson/nlp/Chapter_06_Transformer_Architecture/checkpoints_transformer/transformer-best-epoch=05-val_loss=1.73-val_bleu=0.3321.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at /home/giangson/nlp/Chapter_06_Transformer_Architecture/checkpoints_transformer/transformer-best-epoch=05-val_loss=1.73-val_bleu=0.3321.ckpt\n",
      "/home/giangson/anaconda3/envs/nlp_poetry/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22c1365cb34b8d8dc9db5b13b04390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\">        Test metric        </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_bleu         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.3290563225746155     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    1.7571927309036255     </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test_bleu        \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.3290563225746155    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   1.7571927309036255    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: [{'test_loss': 1.7571927309036255, 'test_bleu': 0.3290563225746155}]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Find the best checkpoint path saved by the callback\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    print(f\"Loading best model for testing from: {best_model_path}\")\n",
    "    # Run the test loop using the best checkpoint\n",
    "    test_result = trainer.test(\n",
    "        model, datamodule=datamodule, ckpt_path=best_model_path\n",
    "    )\n",
    "    print(\"Test Results:\", test_result)\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: No best model checkpoint found or path invalid. Testing with the model's current state (last epoch).\"\n",
    "    )\n",
    "    # If no best model path, test with the model state after the last training epoch\n",
    "    test_result = trainer.test(model, datamodule=datamodule)\n",
    "    print(\"Test Results:\", test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4aef830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Translations (using best model) ---\n",
      "Loading the best model for translation...\n",
      "Setting model to evaluation mode and moving to device...\n",
      "\n",
      "Generating translations for test samples...\n",
      "Source:      Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "Target:      A man in an orange hat starring at something.\n",
      "Predicted:   a man in an orange hat is mixing something .\n",
      "--------------------\n",
      "Source:      Ein Boston Terrier l√§uft √ºber saftig-gr√ºnes Gras vor einem wei√üen Zaun.\n",
      "Target:      A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "Predicted:   a boston - haired female baseball player runs over a white fence .\n",
      "--------------------\n",
      "Source:      Ein M√§dchen in einem Karateanzug bricht ein Brett mit einem Tritt.\n",
      "Target:      A girl in karate uniform breaking a stick with a front kick.\n",
      "Predicted:   a girl in a karate uniform is using a board with a toy .\n",
      "--------------------\n",
      "Source:      F√ºnf Leute in Winterjacken und mit Helmen stehen im Schnee mit Schneemobilen im Hintergrund.\n",
      "Target:      Five people wearing winter jackets and helmets stand in the snow, with snowmobiles in the background.\n",
      "Predicted:   five people in jackets and helmets stand in the snow with snow in the background .\n",
      "--------------------\n",
      "Source:      Leute Reparieren das Dach eines Hauses.\n",
      "Target:      People are fixing the roof of a house.\n",
      "Predicted:   people are fixing the roof of a house .\n",
      "--------------------\n",
      "\n",
      "--- Manual Translation Example ---\n",
      "Manual Source: Ein M√§dchen in einem roten Kleid spielt auf einer Wiese.\n",
      "Generating translation for manual sentence...\n",
      "Predicted:     a girl in a red dress plays on a grass field .\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# --- Translation Examples ---\n",
    "print(\"\\n--- Sample Translations (using best model) ---\")\n",
    "print(\"Loading the best model for translation...\")\n",
    "# Load the best model specifically for inference if needed, or use the one from trainer.test\n",
    "final_model = model  # Use the model loaded/used by trainer.test\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    # Ensure the model loaded has the necessary vocabs etc. for translation\n",
    "    final_model = TransformerLightningModule.load_from_checkpoint(\n",
    "        best_model_path,\n",
    "        # Need to provide args again if not saved in hparams, or if they were ignored\n",
    "        de_vocab=datamodule.de_vocab,\n",
    "        en_vocab=datamodule.en_vocab,\n",
    "        tokenize_de=datamodule.tokenize_de,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using model's last state for final translations.\")\n",
    "\n",
    "print(\"Setting model to evaluation mode and moving to device...\")\n",
    "final_model.freeze()  # Set model to evaluation mode (important!)\n",
    "# Ensure the model is on the correct device (especially if loading manually)\n",
    "final_model.to(\n",
    "    torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get a few samples from the original test data iterator\n",
    "print(\"\\nGenerating translations for test samples...\")\n",
    "_, _, test_iter_raw = Multi30k(\n",
    "    root=config[\"data_dir\"],\n",
    "    split=(\"train\", \"valid\", \"test\"),\n",
    "    language_pair=(\"de\", \"en\"),\n",
    ")\n",
    "test_samples_raw = list(test_iter_raw)[:5]\n",
    "\n",
    "for src_text, tgt_text in test_samples_raw:\n",
    "    # Use the translate_sentence method from our Lightning Module\n",
    "    predicted_text = final_model.translate_sentence(src_text)\n",
    "    print(f\"Source:      {src_text}\")\n",
    "    print(f\"Target:      {tgt_text}\")\n",
    "    print(f\"Predicted:   {predicted_text}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Example with a sentence not from the dataset\n",
    "print(\"\\n--- Manual Translation Example ---\")\n",
    "manual_sentence = \"Ein M√§dchen in einem roten Kleid spielt auf einer Wiese.\"\n",
    "print(f\"Manual Source: {manual_sentence}\")\n",
    "print(\"Generating translation for manual sentence...\")\n",
    "predicted_translation = final_model.translate_sentence(manual_sentence)\n",
    "print(f\"Predicted:     {predicted_translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
